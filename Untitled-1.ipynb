{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution Analysis:\n",
      "fire:\n",
      "  Samples: 48\n",
      "  Ratio: 0.169\n",
      "  Significance Score: 1.623\n",
      "flood:\n",
      "  Samples: 47\n",
      "  Ratio: 0.165\n",
      "  Significance Score: 1.556\n",
      "structural damage:\n",
      "  Samples: 46\n",
      "  Ratio: 0.162\n",
      "  Significance Score: 1.490\n",
      "storm:\n",
      "  Samples: 43\n",
      "  Ratio: 0.151\n",
      "  Significance Score: 1.302\n",
      "earthquake:\n",
      "  Samples: 27\n",
      "  Ratio: 0.095\n",
      "  Significance Score: 0.513\n",
      "surface damage:\n",
      "  Samples: 15\n",
      "  Ratio: 0.053\n",
      "  Significance Score: 0.158\n",
      "tornado:\n",
      "  Samples: 15\n",
      "  Ratio: 0.053\n",
      "  Significance Score: 0.158\n",
      "leak:\n",
      "  Samples: 14\n",
      "  Ratio: 0.049\n",
      "  Significance Score: 0.138\n",
      "mold:\n",
      "  Samples: 10\n",
      "  Ratio: 0.035\n",
      "  Significance Score: 0.070\n",
      "tree:\n",
      "  Samples: 8\n",
      "  Ratio: 0.028\n",
      "  Significance Score: 0.045\n",
      "landslide:\n",
      "  Samples: 5\n",
      "  Ratio: 0.018\n",
      "  Significance Score: 0.018\n",
      "abandoned:\n",
      "  Samples: 2\n",
      "  Ratio: 0.007\n",
      "  Significance Score: 0.003\n",
      "snow:\n",
      "  Samples: 2\n",
      "  Ratio: 0.007\n",
      "  Significance Score: 0.003\n",
      "lava:\n",
      "  Samples: 1\n",
      "  Ratio: 0.004\n",
      "  Significance Score: 0.001\n",
      "sand:\n",
      "  Samples: 1\n",
      "  Ratio: 0.004\n",
      "  Significance Score: 0.001\n",
      "\n",
      "Selected 8 significant classes\n",
      "Selected classes: ['fire', 'flood', 'structural damage', 'storm', 'earthquake', 'surface damage', 'tornado', 'leak']\n",
      "\n",
      "Final dataset sizes:\n",
      "Training samples: 182\n",
      "Validation samples: 36\n",
      "Test samples: 37\n",
      "\n",
      "Final dataset sizes:\n",
      "Training samples: 182\n",
      "Validation samples: 36\n",
      "Test samples: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Documents\\Haclytics2025_AssurantChallenge1\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Documents\\Haclytics2025_AssurantChallenge1\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 3.2250\n",
      "Train Disaster Acc: 0.1538\n",
      "Train Severity Acc: 0.3681\n",
      "Val Loss: 2.8290\n",
      "Val Disaster Acc: 0.3056\n",
      "Val Severity Acc: 0.5556\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 2.7415\n",
      "Train Disaster Acc: 0.3626\n",
      "Train Severity Acc: 0.6374\n",
      "Val Loss: 2.5440\n",
      "Val Disaster Acc: 0.4722\n",
      "Val Severity Acc: 0.6667\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 2.5131\n",
      "Train Disaster Acc: 0.4011\n",
      "Train Severity Acc: 0.6374\n",
      "Val Loss: 2.3976\n",
      "Val Disaster Acc: 0.5556\n",
      "Val Severity Acc: 0.7222\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 2.1503\n",
      "Train Disaster Acc: 0.4670\n",
      "Train Severity Acc: 0.7198\n",
      "Val Loss: 2.3151\n",
      "Val Disaster Acc: 0.5278\n",
      "Val Severity Acc: 0.7778\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 1.8589\n",
      "Train Disaster Acc: 0.5934\n",
      "Train Severity Acc: 0.7857\n",
      "Val Loss: 2.2832\n",
      "Val Disaster Acc: 0.5278\n",
      "Val Severity Acc: 0.7222\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 1.5840\n",
      "Train Disaster Acc: 0.6978\n",
      "Train Severity Acc: 0.8022\n",
      "Val Loss: 2.2939\n",
      "Val Disaster Acc: 0.4722\n",
      "Val Severity Acc: 0.6944\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 1.3277\n",
      "Train Disaster Acc: 0.6868\n",
      "Train Severity Acc: 0.8187\n",
      "Val Loss: 2.2362\n",
      "Val Disaster Acc: 0.4722\n",
      "Val Severity Acc: 0.7500\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 1.1125\n",
      "Train Disaster Acc: 0.7692\n",
      "Train Severity Acc: 0.8846\n",
      "Val Loss: 2.1792\n",
      "Val Disaster Acc: 0.5556\n",
      "Val Severity Acc: 0.7500\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.8897\n",
      "Train Disaster Acc: 0.8242\n",
      "Train Severity Acc: 0.8901\n",
      "Val Loss: 2.1717\n",
      "Val Disaster Acc: 0.5833\n",
      "Val Severity Acc: 0.7500\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.7656\n",
      "Train Disaster Acc: 0.8297\n",
      "Train Severity Acc: 0.9396\n",
      "Val Loss: 2.3058\n",
      "Val Disaster Acc: 0.6389\n",
      "Val Severity Acc: 0.7222\n",
      "\n",
      "Test Results:\n",
      "Test Disaster Accuracy: 0.4054\n",
      "Test Severity Accuracy: 0.5676\n",
      "\n",
      "Disaster Type Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       earthquake       0.67      0.50      0.57         4\n",
      "             fire       0.71      0.71      0.71         7\n",
      "            flood       0.30      0.43      0.35         7\n",
      "             leak       0.50      0.50      0.50         2\n",
      "            storm       0.17      0.17      0.17         6\n",
      "structural damage       0.50      0.14      0.22         7\n",
      "   surface damage       1.00      0.50      0.67         2\n",
      "          tornado       0.17      0.50      0.25         2\n",
      "\n",
      "         accuracy                           0.41        37\n",
      "        macro avg       0.50      0.43      0.43        37\n",
      "     weighted avg       0.48      0.41      0.41        37\n",
      "\n",
      "\n",
      "Severity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.00      0.00      0.00         2\n",
      "      Medium       0.46      0.43      0.44        14\n",
      "        High       0.62      0.71      0.67        21\n",
      "\n",
      "    accuracy                           0.57        37\n",
      "   macro avg       0.36      0.38      0.37        37\n",
      "weighted avg       0.53      0.57      0.55        37\n",
      "\n",
      "\n",
      "Feature Importance Analysis:\n",
      "earthquake:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 4\n",
      "fire:\n",
      "  Accuracy: 0.7143\n",
      "  Samples: 7\n",
      "flood:\n",
      "  Accuracy: 0.4286\n",
      "  Samples: 7\n",
      "leak:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "storm:\n",
      "  Accuracy: 0.1667\n",
      "  Samples: 6\n",
      "structural damage:\n",
      "  Accuracy: 0.1429\n",
      "  Samples: 7\n",
      "surface damage:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "tornado:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "\n",
      "Problematic classes that might need more data or feature engineering:\n",
      "- flood (Accuracy: 0.4286)\n",
      "- storm (Accuracy: 0.1667)\n",
      "- structural damage (Accuracy: 0.1429)\n",
      "\n",
      "Disaster Type Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       earthquake       0.67      0.50      0.57         4\n",
      "             fire       0.71      0.71      0.71         7\n",
      "            flood       0.30      0.43      0.35         7\n",
      "             leak       0.50      0.50      0.50         2\n",
      "            storm       0.17      0.17      0.17         6\n",
      "structural damage       0.50      0.14      0.22         7\n",
      "   surface damage       1.00      0.50      0.67         2\n",
      "          tornado       0.17      0.50      0.25         2\n",
      "\n",
      "         accuracy                           0.41        37\n",
      "        macro avg       0.50      0.43      0.43        37\n",
      "     weighted avg       0.48      0.41      0.41        37\n",
      "\n",
      "\n",
      "Severity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.00      0.00      0.00         2\n",
      "      Medium       0.46      0.43      0.44        14\n",
      "        High       0.62      0.71      0.67        21\n",
      "\n",
      "    accuracy                           0.57        37\n",
      "   macro avg       0.36      0.38      0.37        37\n",
      "weighted avg       0.53      0.57      0.55        37\n",
      "\n",
      "\n",
      "Feature Importance Analysis:\n",
      "earthquake:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 4\n",
      "fire:\n",
      "  Accuracy: 0.7143\n",
      "  Samples: 7\n",
      "flood:\n",
      "  Accuracy: 0.4286\n",
      "  Samples: 7\n",
      "leak:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "storm:\n",
      "  Accuracy: 0.1667\n",
      "  Samples: 6\n",
      "structural damage:\n",
      "  Accuracy: 0.1429\n",
      "  Samples: 7\n",
      "surface damage:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "tornado:\n",
      "  Accuracy: 0.5000\n",
      "  Samples: 2\n",
      "\n",
      "Saving disaster types to file...\n",
      "Disaster types saved successfully\n"
     ]
    }
   ],
   "source": [
    "def analyze_class_performance(labels_true, labels_pred, class_names):\n",
    "    \"\"\"Analyze per-class performance metrics\"\"\"\n",
    "    performance = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = labels_true == i\n",
    "        if np.sum(mask) > 0:\n",
    "            accuracy = np.mean(labels_pred[mask] == labels_true[mask])\n",
    "            performance[class_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'samples': np.sum(mask)\n",
    "            }\n",
    "    return performance\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, data_df, img_dir, transform=None):\n",
    "        self.data_df = data_df.copy()\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.disaster_types = sorted(data_df['Disaster Type'].unique())\n",
    "        self.disaster2idx = {disaster: idx for idx, disaster in enumerate(self.disaster_types)}\n",
    "        \n",
    "        # Calculate class weights for disaster types\n",
    "        disaster_counts = data_df['Disaster Type'].value_counts()\n",
    "        total_samples = len(data_df)\n",
    "        self.disaster_weights = torch.FloatTensor([\n",
    "            total_samples / (len(disaster_counts) * count) \n",
    "            for disaster in self.disaster_types\n",
    "            for count in [disaster_counts[disaster]]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['Image Name'])\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get labels\n",
    "        disaster_label = self.disaster2idx[row['Disaster Type']]\n",
    "        severity_label = int(row['Severity'])\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'disaster_type': disaster_label,\n",
    "            'severity': severity_label,\n",
    "            'weight': self.disaster_weights[disaster_label]\n",
    "        }\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_disaster_types):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Use ResNet50 backbone\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Task-specific heads with dropout\n",
    "        self.disaster_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_disaster_types)\n",
    "        )\n",
    "        \n",
    "        self.severity_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).squeeze(-1).squeeze(-1)\n",
    "        disaster_out = self.disaster_classifier(features)\n",
    "        severity_out = self.severity_classifier(features)\n",
    "        return disaster_out, severity_out\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    disaster_correct = 0\n",
    "    severity_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        disaster_labels = batch['disaster_type'].to(device)\n",
    "        severity_labels = batch['severity'].to(device)\n",
    "        weights = batch['weight'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        disaster_out, severity_out = model(images)\n",
    "        \n",
    "        # Calculate weighted disaster loss - ensure it's a scalar\n",
    "        disaster_loss = criterion(disaster_out, disaster_labels)\n",
    "        disaster_loss = (disaster_loss * weights).mean()  # Reduce to scalar\n",
    "        \n",
    "        # Calculate severity loss - ensure it's a scalar\n",
    "        severity_loss = criterion(severity_out, severity_labels).mean()  # Reduce to scalar\n",
    "        \n",
    "        # Combined loss (now guaranteed to be a scalar)\n",
    "        loss = disaster_loss + severity_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        with torch.no_grad():\n",
    "            disaster_pred = disaster_out.argmax(dim=1)\n",
    "            severity_pred = severity_out.argmax(dim=1)\n",
    "            disaster_correct += (disaster_pred == disaster_labels).sum().item()\n",
    "            severity_correct += (severity_pred == severity_labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(train_loader),\n",
    "        'disaster_acc': disaster_correct / total_samples,\n",
    "        'severity_acc': severity_correct / total_samples\n",
    "    }\n",
    "\n",
    "def evaluate(model, data_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    disaster_correct = 0\n",
    "    severity_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_disaster_preds = []\n",
    "    all_severity_preds = []\n",
    "    all_disaster_labels = []\n",
    "    all_severity_labels = []\n",
    "    \n",
    "    # Initialize per-class metrics\n",
    "    class_correct = {}\n",
    "    class_total = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            disaster_labels = batch['disaster_type'].to(device)\n",
    "            severity_labels = batch['severity'].to(device)\n",
    "            \n",
    "            disaster_out, severity_out = model(images)\n",
    "            \n",
    "            # Calculate losses - ensure they're scalars\n",
    "            disaster_loss = criterion(disaster_out, disaster_labels).mean()  # Reduce to scalar\n",
    "            severity_loss = criterion(severity_out, severity_labels).mean()  # Reduce to scalar\n",
    "            loss = disaster_loss + severity_loss  # Now guaranteed to be a scalar\n",
    "            \n",
    "            # Track predictions\n",
    "            disaster_pred = disaster_out.argmax(dim=1)\n",
    "            severity_pred = severity_out.argmax(dim=1)\n",
    "            \n",
    "            disaster_correct += (disaster_pred == disaster_labels).sum().item()\n",
    "            severity_correct += (severity_pred == severity_labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_disaster_preds.extend(disaster_pred.cpu().numpy())\n",
    "            all_severity_preds.extend(severity_pred.cpu().numpy())\n",
    "            all_disaster_labels.extend(disaster_labels.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'disaster_acc': disaster_correct / total_samples,\n",
    "        'severity_acc': severity_correct / total_samples,\n",
    "        'predictions': {\n",
    "            'disaster': np.array(all_disaster_preds),\n",
    "            'severity': np.array(all_severity_preds)\n",
    "        },\n",
    "        'labels': {\n",
    "            'disaster': np.array(all_disaster_labels),\n",
    "            'severity': np.array(all_severity_labels)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    RANDOM_SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Set random seeds\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    \n",
    "    # Data paths\n",
    "    img_dir = \"images\"\n",
    "    csv_path = \"disasterData/disaster_analysis_filename_based.csv\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Analyze class distribution and significance\n",
    "    class_counts = df['Disaster Type'].value_counts()\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    # Calculate class significance scores\n",
    "    significance_scores = {}\n",
    "    min_class_ratio = 0.02  # Minimum 2% of total samples\n",
    "    min_absolute_samples = 5  # Minimum absolute number of samples\n",
    "    \n",
    "    print(\"\\nClass Distribution Analysis:\")\n",
    "    for disaster_type, count in class_counts.items():\n",
    "        ratio = count / total_samples\n",
    "        significance_score = ratio * (count / min_absolute_samples)\n",
    "        significance_scores[disaster_type] = significance_score\n",
    "        \n",
    "        print(f\"{disaster_type}:\")\n",
    "        print(f\"  Samples: {count}\")\n",
    "        print(f\"  Ratio: {ratio:.3f}\")\n",
    "        print(f\"  Significance Score: {significance_score:.3f}\")\n",
    "    \n",
    "    # Select significant classes\n",
    "    significance_threshold = 0.1\n",
    "    valid_classes = [\n",
    "        cls for cls, score in significance_scores.items()\n",
    "        if score >= significance_threshold and class_counts[cls] >= min_absolute_samples\n",
    "    ]\n",
    "    \n",
    "    # Create separate dataframes for each class\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    \n",
    "    for disaster_type in valid_classes:\n",
    "        # Get data for this class\n",
    "        class_data = df[df['Disaster Type'] == disaster_type]\n",
    "        n_samples = len(class_data)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        n_test = max(2, int(0.3 * n_samples))  # At least 2 samples for test\n",
    "        \n",
    "        # Random split\n",
    "        class_test = class_data.sample(n=n_test, random_state=RANDOM_SEED)\n",
    "        class_train = class_data.drop(class_test.index)\n",
    "        \n",
    "        train_df = pd.concat([train_df, class_train])\n",
    "        test_df = pd.concat([test_df, class_test])\n",
    "    \n",
    "    # Final split of test into val and test\n",
    "    val_df, test_df = train_test_split(\n",
    "        test_df, \n",
    "        test_size=0.5, \n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=test_df['Disaster Type']\n",
    "    )\n",
    "    \n",
    "    # Shuffle all datasets\n",
    "    train_df = train_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nSelected {len(valid_classes)} significant classes\")\n",
    "    print(\"Selected classes:\", valid_classes)\n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Ensure at least 2 samples per class for train/test split\n",
    "    train_size = 0.7  # 70% for training\n",
    "    min_test_size = 2  # Minimum 2 samples per class in test set\n",
    "    \n",
    "    # Custom split to ensure minimum samples\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    \n",
    "    for disaster_type in valid_classes:\n",
    "        class_data = df[df['Disaster Type'] == disaster_type]\n",
    "        n_samples = len(class_data)\n",
    "        n_test = max(min_test_size, int(n_samples * (1 - train_size)))\n",
    "        \n",
    "        # Random split within each class\n",
    "        class_test = class_data.sample(n=n_test, random_state=RANDOM_SEED)\n",
    "        class_train = class_data.drop(class_test.index)\n",
    "        \n",
    "        train_df = pd.concat([train_df, class_train])\n",
    "        test_df = pd.concat([test_df, class_test])\n",
    "    \n",
    "    # Further split test into val and test\n",
    "    val_df, test_df = train_test_split(\n",
    "        test_df, \n",
    "        test_size=0.5, \n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=test_df['Disaster Type']\n",
    "    )\n",
    "    \n",
    "    # Shuffle the final datasets\n",
    "    train_df = train_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = DisasterDataset(train_df, img_dir, transform=train_transform)\n",
    "    val_dataset = DisasterDataset(val_df, img_dir, transform=eval_transform)\n",
    "    test_dataset = DisasterDataset(test_df, img_dir, transform=eval_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Setup model and training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiTaskModel(num_disaster_types=len(train_dataset.disaster_types))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, device, criterion)\n",
    "        val_metrics = evaluate(model, val_loader, device, criterion)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['disaster_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['disaster_acc']\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(f\"Train Disaster Acc: {train_metrics['disaster_acc']:.4f}\")\n",
    "        print(f\"Train Severity Acc: {train_metrics['severity_acc']:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val Disaster Acc: {val_metrics['disaster_acc']:.4f}\")\n",
    "        print(f\"Val Severity Acc: {val_metrics['severity_acc']:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_metrics = evaluate(model, test_loader, device, criterion)\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Test Disaster Accuracy: {test_metrics['disaster_acc']:.4f}\")\n",
    "    print(f\"Test Severity Accuracy: {test_metrics['severity_acc']:.4f}\")\n",
    "    \n",
    "    # Calculate classification reports with zero_division parameter\n",
    "    disaster_report = classification_report(\n",
    "        test_metrics['labels']['disaster'],\n",
    "        test_metrics['predictions']['disaster'],\n",
    "        target_names=train_dataset.disaster_types,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    severity_report = classification_report(\n",
    "        test_metrics['labels']['severity'],\n",
    "        test_metrics['predictions']['severity'],\n",
    "        target_names=['Low', 'Medium', 'High'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDisaster Type Classification Report:\")\n",
    "    print(disaster_report)\n",
    "    \n",
    "    print(\"\\nSeverity Classification Report:\")\n",
    "    print(severity_report)\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    class_performance = {}\n",
    "    for i, disaster_type in enumerate(train_dataset.disaster_types):\n",
    "        mask = test_metrics['labels']['disaster'] == i\n",
    "        if np.sum(mask) > 0:  # Only calculate if we have samples\n",
    "            accuracy = np.mean(\n",
    "                test_metrics['predictions']['disaster'][mask] == \n",
    "                test_metrics['labels']['disaster'][mask]\n",
    "            )\n",
    "            class_performance[disaster_type] = accuracy\n",
    "            print(f\"{disaster_type}:\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Samples: {np.sum(mask)}\")\n",
    "    \n",
    "    # Identify problematic classes\n",
    "    problem_threshold = 0.7  # Adjust based on your needs\n",
    "    problematic_classes = [\n",
    "        cls for cls, acc in class_performance.items()\n",
    "        if acc < problem_threshold\n",
    "    ]\n",
    "    \n",
    "    if problematic_classes:\n",
    "        print(\"\\nProblematic classes that might need more data or feature engineering:\")\n",
    "        for cls in problematic_classes:\n",
    "            print(f\"- {cls} (Accuracy: {class_performance[cls]:.4f})\")\n",
    "    print(\"\\nDisaster Type Classification Report:\")\n",
    "    print(disaster_report)\n",
    "    \n",
    "    print(\"\\nSeverity Classification Report:\")\n",
    "    print(severity_report)\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    class_performance = {}\n",
    "    for i, disaster_type in enumerate(train_dataset.disaster_types):\n",
    "        mask = test_metrics['labels']['disaster'] == i\n",
    "        if np.sum(mask) > 0:\n",
    "            accuracy = np.mean(\n",
    "                test_metrics['predictions']['disaster'][mask] == \n",
    "                test_metrics['labels']['disaster'][mask]\n",
    "            )\n",
    "            class_performance[disaster_type] = accuracy\n",
    "            print(f\"{disaster_type}:\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Samples: {np.sum(mask)}\")\n",
    "\n",
    "    # Save disaster types to file - Add this here\n",
    "    print(\"\\nSaving disaster types to file...\")\n",
    "    with open('disaster_types.txt', 'w') as f:\n",
    "        for disaster_type in train_dataset.disaster_types:\n",
    "            f.write(f\"{disaster_type}\\n\")\n",
    "    print(\"Disaster types saved successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports to match the training code\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Use the same model architecture as in training\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_disaster_types):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Use ResNet50 backbone\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Task-specific heads with dropout\n",
    "        self.disaster_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_disaster_types)\n",
    "        )\n",
    "        \n",
    "        self.severity_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).squeeze(-1).squeeze(-1)\n",
    "        disaster_out = self.disaster_classifier(features)\n",
    "        severity_out = self.severity_classifier(features)\n",
    "        return disaster_out, severity_out\n",
    "\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model_path, disaster_types):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Store disaster types\n",
    "        self.disaster_types = disaster_types\n",
    "        \n",
    "        # Initialize model with correct number of classes\n",
    "        self.model = MultiTaskModel(num_disaster_types=len(self.disaster_types))\n",
    "        \n",
    "        # Load trained weights\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "            print(\"Model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Use the same transform as in training eval\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.severity_levels = ['Low', 'Medium', 'High']\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocess the input image using training parameters\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            transformed_image = self.transform(image)\n",
    "            return transformed_image.unsqueeze(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Make predictions on a single image\"\"\"\n",
    "        # Preprocess image\n",
    "        image_tensor = self.preprocess_image(image_path)\n",
    "        if image_tensor is None:\n",
    "            return None\n",
    "        \n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            disaster_out, severity_out = self.model(image_tensor)\n",
    "            \n",
    "            # Get probabilities\n",
    "            disaster_probs = torch.softmax(disaster_out, dim=1)\n",
    "            severity_probs = torch.softmax(severity_out, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            disaster_idx = disaster_probs.argmax(dim=1).item()\n",
    "            severity_idx = severity_probs.argmax(dim=1).item()\n",
    "            \n",
    "            # Get confidence scores\n",
    "            disaster_conf = disaster_probs[0][disaster_idx].item()\n",
    "            severity_conf = severity_probs[0][severity_idx].item()\n",
    "            \n",
    "            # Get top-3 disaster predictions\n",
    "            top3_disasters = []\n",
    "            probs, indices = torch.topk(disaster_probs[0], min(3, len(self.disaster_types)))\n",
    "            for prob, idx in zip(probs, indices):\n",
    "                top3_disasters.append({\n",
    "                    'type': self.disaster_types[idx],\n",
    "                    'confidence': prob.item() * 100\n",
    "                })\n",
    "            \n",
    "        return {\n",
    "            'top_predictions': top3_disasters,\n",
    "            'primary_disaster': self.disaster_types[disaster_idx],\n",
    "            'disaster_confidence': disaster_conf * 100,\n",
    "            'severity': self.severity_levels[severity_idx],\n",
    "            'severity_confidence': severity_conf * 100\n",
    "        }\n",
    "\n",
    "def predict_all_images(predictor, image_dir):\n",
    "    \"\"\"\n",
    "    Predict and display results for all images in the directory\n",
    "    \"\"\"\n",
    "    # Get all image files\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "    image_files = [f for f in os.listdir(image_dir) \n",
    "                  if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_images = len(image_files)\n",
    "    n_cols = min(3, n_images)  # Max 3 images per row\n",
    "    n_rows = math.ceil(n_images / n_cols)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(6*n_cols, 6*n_rows))\n",
    "    \n",
    "    # Process each image\n",
    "    all_predictions = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files, 1):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        \n",
    "        # Create subplot\n",
    "        plt.subplot(n_rows, n_cols, idx)\n",
    "        \n",
    "        # Display image\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Make prediction\n",
    "        results = predictor.predict(image_path)\n",
    "        \n",
    "        if results:\n",
    "            # Store prediction results\n",
    "            all_predictions.append({\n",
    "                'image': image_file,\n",
    "                'predictions': results\n",
    "            })\n",
    "            \n",
    "            # Format title with main prediction and confidence\n",
    "            title = f\"{results['primary_disaster']}\\n\"\n",
    "            title += f\"Confidence: {results['disaster_confidence']:.1f}%\\n\"\n",
    "            title += f\"Severity: {results['severity']}\"\n",
    "            \n",
    "            plt.title(title, fontsize=10, pad=10)\n",
    "            \n",
    "            # Print detailed results\n",
    "            print(f\"\\nResults for {image_file}:\")\n",
    "            print(\"Top predictions:\")\n",
    "            for pred in results['top_predictions']:\n",
    "                print(f\"- {pred['type']}: {pred['confidence']:.1f}%\")\n",
    "            print(f\"Severity: {results['severity']} ({results['severity_confidence']:.1f}% confidence)\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Example usage in notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
